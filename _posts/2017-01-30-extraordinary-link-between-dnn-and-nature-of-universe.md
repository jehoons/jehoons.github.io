---
title: 'The extraordinary link between deep neural networks and the nature of the universe'
date: '2017-1-30 10:00'
layout: post
published: true 
---

복잡한 문제들을 푸는데 딥뉴럴넷이 왜 그렇게 좋은 성능을 보일 수 있을까요? 최근, 물리학자들은 물리적 법칙에서 그 비밀을 찾았습니다. 지난 몇 년 동안, 딥러닝 기술은 인공지능 분야를 변화시켜 왔습니다. 하나씩 하나씩, 인간이 자신의 고유한 능력과 기술이라고 생각해 왔던 여러가지 것들이 강력해진 기계들의 맹공격을 받기 시작하였습니다. 얼굴 인식 및 물체 인식과 같은 작업에서 딥뉴럴넷은 인간보다 뛰어납니다. 그들은 고대부터 전해져온 바둑을 마스터하였고 최고의 인간 플레이어를 제압했습니다. 

> 이 문서의 원문은 [여기](https://www.technologyreview.com/s/602344/the-extraordinary-link-between-deep-neural-networks-and-the-nature-of-the-universe)를 확인하세요.

하지만 문제가 있습니다. 여기에는 그 어떤 수학적인 이유도 없습니다. 

그러나 문제가 있습니다. 레이어로 배열 된 네트워크가 이러한 문제를 잘 처리해야 하는지에 대한 그 어떤 수학적 이유도 없습니다. 수학자는 단지 어리동절할 뿐입니다. 딥뉴럴네트워크의 커다란 성공에도 불구하고 아무도 이것이 어떻게 그것을 달성하였는지에 대해서는 확신할 수 없었습니다. 

하버드 대학의 Henry Lin과 MIT대학의 Max Tegmark의 연구 덕분에 이러한 상황은 바뀌었습니다. 이들은 그 답이 우주의 본성에 달려 있기대문에 수학자들을 당혹스럽게 했다고 말합니다. 즉, 그 답은 수학이 아니라 물리학의 영역에 있다는 말입니다. 

먼저 메가비트크기를 가지는 그레이스케일 이미지를 분류하여 이것이 고양인지 아니면 개를 보여 주는지 여부를 결정하는 문제를 생각해 봅시다.

이러한 이미지는 256개의 그레이스케일 값 중에서 하나를 취할 수 있는 백만픽셀로 구성됩니다. 이론적으로는 2561000000 개의 이미지가 있을 수 있으며 각 이미지마다 고양이 또는 개를 보여줄지 여부를 계산해야 할 것입니다. 하지만, 수천 또는 수백만 개의 매개변수를 가지고 있는 신경망은 어떻게든 이 분류 작업을 쉽게 관리할수 있습니다.

수학적 언어로 보자면, 신경망은 복잡한 수학함수를 보다 간단한 것으로 근사함으로써 작동합니다. 고양이와 개의 이미지들을 분류할때, 신경망은 백만개의 그레이스케일 픽셀을 입력으로 받아서 그것이 표현하는 것이 무엇인지를 확률 분포로써 출력하는 함수를 구현해야 합니다.

여기서 문제는 이것을 구현하는데 가능한 네트워크의 갯수보다 근사화된 수학함수의 갯수가 훨씬 더 많다는 것입니다. 그럼에도 불구하고 딥뉴럴네트워크는 어떻게든 올바른 답을 얻어냅니다.

이제 린과 테그마크는 그들이 효과가 있는 이유가 무엇인지 말합니다. 그 대답은 우주가 모든 가능한 함수들 중에서 아주 작은 부분집합에 의해서 지배가 된다는 것입니다. 즉, 물리학의 법칙이 수식으로 기록될때, 이들은 모두 단순한 속성을 지닌 주목할 만한 셋을 가지는 수학함수에 의해서 모두 기술 될 수 있습니다.

그러므로 딥뉴럴네트워크들은 가능한 수학적 함수를 근사할 필요는 없으며, 단지 그것들의 작은 부분집합 일뿐입니다.

이것을 원근법으로 생각해보기 위해서, 가장 높은 지수의 크기인 다항식 함수의 차수를 고려해 봅시다. 그러면 $$y = x^{2}$$와 같은 2차 방정식은 2차수를 가지며, $$y = x^{24}$$방정식은 24차수를 갖습니다.

분명히, 오더의 수는 무한하지만, 물리 법칙에서는 다항식의 작은 부분집합만이 출현합니다. "아직 완전히 이해되지는 않은 이유 때문에 우리의 우주는 낮은 차수의 다항식 해밀토니안에 의해서 정확하게 기술될 수가 있습니다"라고 Lin과 Tegmark는 말합니다. 일반적으로 물리 법칙을 설명하는 다항식은 2에서 4까지의 차수를 가집니다.

또한 물리 법칙은 다른 중요한 특성을 가지고 있습니다. 예를 들어, 회전 및 평행 이동과 관련한 대칭입니다. 고양이나 개를 360도로 회전 시키면 똑같이 보입니다. 10미터 또는 100미터 또는 1킬로미터로 평행 이동하여도 동일하게 보입니다. 이는 또한 고양이와 개에 대한 인식 과정을 근사화시키는 작업을 단순화 한다고 볼 수 있습니다.

이러한 속성은 신경망이 가능한 무한히 많은 수학 함수를 근사하지 않아도 되지만, 가장 단순한 함수의 작은 부분만을 근사화할 필요가 있음을 의미합니다.

신경망이 이용하는 우주의 또 다른 속성이 있습니다. 이것은 구조의 계층 구조입니다. "초등적인 입자는 원자, 분자, 세포, 유기체, 행성, 태양계, 은하 등을 형성합니다."라고 린과 테그마크는 말합니다. 복잡한 구조는 일련의 간단한 단계를 통해서 형성되는 경우가 많습니다.

이것이 신경망의 구조가 중요한 이유이기도합니다.이 네트워크의 계층은 인과 관계 순서의 각 단계를 근사 할 수 있습니다.

린과 테그마크는 우주의 마이크로 웨이브 배경 방사선의 예를 들었는데 이것은 우주에 침투하는 빅뱅의 메아리입니다. 최근 몇 년 동안 다양한 우주선이 이 방사선을 보다 높은 해상도로 매핑했습니다. 그리고 물론, 물리학자들은 왜 이 지도가 그러한 형태를 취하는 지에 대해서 궁금해 하였습니다.


(계속)
