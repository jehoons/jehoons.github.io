---
title: 'LSTM 네트워크의 이해'
date: '2017-1-4 10:00'
layout: post
published: true
---

> 이 문서의 원문은 [여기](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)에서 확인할 수 있습니다. 꼼꼼히 읽어보기 위해서 번역합니다.

## LSTM 네트워크의 이해

### Recurrent Neural Networks

당신은 매 초마다 새로운 생각을 시작하는 것은 아니다. 이 글을 읽는 동안 당신은 이전 단어의 이해에 기반을 두어 단어들을 이해한다. 즉 당신은 모든 생각을 던져 버리고 새로운 생각을 시작하지는 않는다는 것이다. 그러므로 당신의 생각은 지속성을 가지고 있다. 

전통적 신경망은 이것을 할 수 없으며, 이는 주요한 결점인 것 처럼 보인다. 예를 들자면, 영화를 관람할때 매 시간마다 어떤 종류의 이벤트가 발생하는지를 분류하려고 한다고 상상해 보자. 전통적인 뉴럴네트워크가 어떻게 영화의 이전 사건에 대한 추론을 사용하여 나중에 알려줄 (분류할 수) 수 있는지 불분명하다.

재귀신경망은 이 문제를 처리할 수 있다. 재귀신경망들은 내부에 피드백을 가지고 있으며, 그로인해 정보가 지속성을 가지도록 할수 있다.

<div style="text-align:center" markdown="1">
![](https://www.dropbox.com/s/3cxzh6z34utx9qw/RNN-rolled.png?dl=1){:height="200px" .center-image}

**그림.** 루프를 가지고 있는 재귀신경망
</div>

위 다이어그램에서, 신경망 묶음 $$A$$는 입력 $$X_t$$를 받아들이고 $$h_t$$를 출력으로 내보낸다. 여기서 루프는 정보가 네트워크의 하나의 단계로부터 네트워크의 다음으로 통과하도록 허용한다.

이러한 루프들은 재귀신경망을 다소 신비하게 보이도록 한다. 반면, 조금 더 생각해보면, 보통의 뉴럴네트워크와 완전히 다르지는 않다는 것을 알게 된다. 재귀신경망은 동일한 네트워크의 여러 복사본으로 생각할수 있으며 각각은 후임자 네트워크에게 메시지를 전달한다. 루프를 푼다는 어떻게 될지 생각해 보자: 

<div style="text-align:center" markdown="1">
![](https://www.dropbox.com/s/9n9r2ro3s5itb7c/RNN-unrolled.png?dl=1){:height="200px" .center-image}

**그림.** 루프를 푼 재귀신경망
</div>

이러한 재귀신경망의 체인같은 성질은 재귀신경망이 시퀀스와 리스트 데이터에 밀접하게 관련되어 있다는 것을 의미한다. 즉, 시퀀스와 리스트 데이터에 적용할수 있는 신경망의 자연스러운 구조라는 것이다. 과연 그런가? 

이들은 확실히 사용되고 있다! 지난 몇년 동안 음성인식, 언어모델링, 번역, 이미지 캡션과 같은 다양한 문제들에 대해해 RNN이 성공적으로 적용된다. 계속해서 Andrej Karpathy의 훌륭한 블로그 게시물인 Reason Neural Networks의 비상식적으로 (좋은) 효과를 살펴보면서, RNN으로 함께 달성할 수 있는 놀라운 업적에 대해서 논의한다. 실제로 그것들은 진짜로 훌륭히 일을 할 수 있다. 

이러한 성공의 핵심은 "LSTMs"를 사용하는 것인데, 이것은 매우 특별한 종류의 재귀적 신경망이며 많은 작업에 대해서 표준 버전보다 훨씬 효과적으로 작동한다. 재귀적 신경망을 기반으로 한 거의 모든 흥미 진진한 결과는 이들과 함께 달성된다. 그것이 바로 이 에세이에서 살펴볼 LSTM들이다.

### 장기적 종속성의 문제(The Problem of Long-Term Dependencies)
RNN의 매력중 한가지는 이전 비디오 프레임을 사용하여 현재 프레임의 이해를 알리는 것처럼 이전의 정보를 현재의 작업에 연결할 수 있다는 생각이다. 만약 이를 수행할 수 있다면 RNN은 매우 유용할 것이다. 이것이 가능할까? 그것은 경우에 따라 다르다.

때로는 현재의 작업을 처리하기 위해서 단지 최근의 정보만을 살펴봐야 할때도 있다. 예를 들어, 이전 단어를 기반으로 다음 단어를 예측하려고 시도하는 언어 모델을 생각해 보자. 우리가 "the clouds are in the sky" 라는 문장에서 마지막 단어를 예측하려고 한다면, 우리는 더 이상의 정보를 필요로 하지 않는다. 다음 단어가 sky가 될 것이란 것은 매우 분명하다. 그러한 경우, 즉 관련된 정보와 그것을 필요로 하는 장소 사이의 간격이 작은 경우에는, RNN은 과거의 정보를 사용하는 방법을 배울 수 있다.

<div style="text-align:center" markdown="1">
![](https://www.dropbox.com/s/ktw35gp24wfob16/RNN-shorttermdepdencies.png?dl=1){:height="200px" .center-image}

**그림.** 단기 종속성의 경우
</div>

그러나, 더 많은 맥락을 필요로 하는 경우들도 있다. "I grew up in France... I speak fluent French." 라는 텍스트에서 마지막 단어를 예측하려고 해보자. 마지막 단어의 최근 정보는 다음에 오는 단어가 아마도 언어의 이름일것이라는 것을 제안할수 있을 것이지만, 어떤 언어인지로 좁히기 위해서는, 우리는 더 뒤로부터 france의 맥락을 필요로 한다. 연관된 정보와 그것이 필요로 해지는 시점간의 간격이 매우 커지게 되는 상황은 전적으로 가능하다. 불행히도 그 격차가 커지면서 RNN은 정보를 연결하는 법을 배울 수 없게 된다.

<div style="text-align:center" markdown="1">
![](https://www.dropbox.com/s/g3v59uu75vwo1td/RNN-longtermdependencies.png?dl=1){:height="200px" .center-image}

**그림.** 장기 종속성의 경우
</div>

이론적으로는 RNN이 이러한 "장기적 의존성"을 절대적으로 처리할 수 있다. 인간은 이러한 형태의 쉬운 문제를 해결하기 위해서 신중히 매개변수를 선택할 수 있다. 그러나 슬프게도.. 실질적으로는, RNN은 그러한 장기적 의존성문제를 해결하도록 학습할 수 있을 것 같지 않다. 이 문제는 Hochreiter (1991) [독일]와 Bengio, et al. (1994)에 의해서 깊이 있게 조사되었는데, 그들은 이 그것이 어려운 매우 근본적인 몇가지 이유를 발견하였다.

고맙게도, LSTM 네트워크에는 이러한 문제가 없다!

### LSTM 네트워크 

Long Short Term 메모리 네트워크 (일반적으로 "LSTM"이라고 함)는 특별한 종류의 RNN이며 장기 의존성을 학습 할 수 있습니다. 그들은 Hochreiter & Schmidhuber (1997)에 의해 소개되었으며 다음과 같은 작업에서 많은 사람들이 세련되고 대중화되었습니다 .1 이들은 다양한 문제에 대해 대단히 잘 작동하며 현재 널리 사용되고 있습니다.

LSTM은 장기 의존성 문제를 피하기 위해 명시 적으로 설계되었습니다. 오랜 기간 동안 정보를 기억하는 것은 실제 배우는 것이 아닌 자신의 기본 행동입니다!

모든 반복적 인 신경망은 신경망의 반복적 인 모듈 체인의 형태를 가진다. 표준 RNN에서이 반복 모듈은 단일 탄층과 같은 매우 간단한 구조를 갖습니다.


(진행중)


