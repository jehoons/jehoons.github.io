---
title: 'LSTM 네트워크의 이해'
date: '2017-1-4 10:00'
layout: post
published: true
---

> 이 문서의 원문은 [여기](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)에서 확인할 수 있습니다. 꼼꼼히 읽어보기 위해서 번역합니다.

## LSTM 네트워크의 이해

### Recurrent Neural Networks

당신은 매 초마다 새로운 생각을 시작하는 것은 아니다. 이 글을 읽는 동안 당신은 이전 단어의 이해에 기반을 두어 단어들을 이해한다. 즉 당신은 모든 생각을 던져 버리고 새로운 생각을 시작하지는 않는다는 것이다. 그러므로 당신의 생각은 지속성을 가지고 있다. 

전통적 신경망은 이것을 할 수 없으며, 이는 주요한 결점인 것 처럼 보인다. 예를 들자면, 영화를 관람할때 매 시간마다 어떤 종류의 이벤트가 발생하는지를 분류하려고 한다고 상상해 보자. 전통적인 뉴럴네트워크가 어떻게 영화의 이전 사건에 대한 추론을 사용하여 나중에 알려줄 (분류할 수) 수 있는지 불분명하다.

재귀신경망은 이 문제를 처리할 수 있다. 재귀신경망들은 내부에 피드백을 가지고 있으며, 그로인해 정보가 지속성을 가지도록 할수 있다.

<div style="text-align:center" markdown="1">
![](https://www.dropbox.com/s/3cxzh6z34utx9qw/RNN-rolled.png?dl=1){:height="200px" .center-image}

**그림.** 루프를 가지고 있는 재귀신경망
</div>

위 다이어그램에서, 신경망 묶음 $$A$$는 입력 $$X_t$$를 받아들이고 $$h_t$$를 출력으로 내보낸다. 여기서 루프는 정보가 네트워크의 하나의 단계로부터 네트워크의 다음으로 통과하도록 허용한다.

이러한 루프들은 재귀신경망을 다소 신비하게 보이도록 한다. 반면, 조금 더 생각해보면, 보통의 뉴럴네트워크와 완전히 다르지는 않다는 것을 알게 된다. 재귀신경망은 동일한 네트워크의 여러 복사본으로 생각할수 있으며 각각은 후임자 네트워크에게 메시지를 전달한다. 루프를 푼다는 어떻게 될지 생각해 보자: 

<div style="text-align:center" markdown="1">
![](https://www.dropbox.com/s/9n9r2ro3s5itb7c/RNN-unrolled.png?dl=1){:height="200px" .center-image}

**그림.** 루프를 푼 재귀신경망
</div>

이러한 재귀신경망의 체인같은 성질은 재귀신경망이 시퀀스와 리스트 데이터에 밀접하게 관련되어 있다는 것을 의미한다. 즉, 시퀀스와 리스트 데이터에 적용할수 있는 신경망의 자연스러운 구조라는 것이다. 과연 그런가? 

이들은 확실히 사용되고 있다! 지난 몇년 동안 음성인식, 언어모델링, 번역, 이미지 캡션과 같은 다양한 문제들에 대해해 RNN이 성공적으로 적용된다. 계속해서 Andrej Karpathy의 훌륭한 블로그 게시물인 Reason Neural Networks의 비상식적으로 (좋은) 효과를 살펴보면서, RNN으로 함께 달성할 수 있는 놀라운 업적에 대해서 논의한다. 실제로 그것들은 진짜로 훌륭히 일을 할 수 있다. 

이러한 성공의 핵심은 "LSTMs"를 사용하는 것인데, 이것은 매우 특별한 종류의 재귀적 신경망이며 많은 작업에 대해서 표준 버전보다 훨씬 효과적으로 작동한다. 재귀적 신경망을 기반으로 한 거의 모든 흥미 진진한 결과는 이들과 함께 달성된다. 그것이 바로 이 에세이에서 살펴볼 LSTM들이다.

### 장기적 종속성의 문제(The Problem of Long-Term Dependencies)
RNN의 매력중 한가지는 이전 비디오 프레임을 사용하여 현재 프레임의 이해를 알리는 것처럼 이전의 정보를 현재의 작업에 연결할 수 있다는 생각이다. 만약 이를 수행할 수 있다면 RNN은 매우 유용할 것이다. 이것이 가능할까? 그것은 경우에 따라 다르다.

때로는 현재의 작업을 처리하기 위해서 단지 최근의 정보만을 살펴봐야 할때도 있다. 예를 들어, 이전 단어를 기반으로 다음 단어를 예측하려고 시도하는 언어 모델을 생각해 보자. 우리가 "the clouds are in the sky" 라는 문장에서 마지막 단어를 예측하려고 한다면, 우리는 더 이상의 정보를 필요로 하지 않는다. 다음 단어가 sky가 될 것이란 것은 매우 분명하다. 그러한 경우, 즉 관련된 정보와 그것을 필요로 하는 장소 사이의 간격이 작은 경우에는, RNN은 과거의 정보를 사용하는 방법을 배울 수 있다.

<div style="text-align:center" markdown="1">
![](https://www.dropbox.com/s/ktw35gp24wfob16/RNN-shorttermdepdencies.png?dl=1){:height="200px" .center-image}

**그림.** 단기 종속성의 경우
</div>


(진행중)



